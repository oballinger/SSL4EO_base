{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0d3f44a-fa17-4d6b-a1d4-a8bfe9ebaa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchsummary \n",
    "\n",
    "!git remote -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6987915-8665-4b00-8001-50e93331dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "#from torchsummary import summary\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89859edb-91e0-4502-ad6d-30ba3d0e6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look into the model's architecure\n",
    "\n",
    "# Load the pre-trained ResNet50 model\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Print the model structure\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d2a79e5-00bd-425f-a8d0-e5307c9b5bec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[1;32m      6\u001b[0m     \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# get random image\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     data \u001b[38;5;241m=\u001b[39m dataset[idx]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Function for loading data without transformations\n",
    "\n",
    "num_samples = 5 \n",
    "\n",
    "def dataloader(dataset):\n",
    "    for i in range(num_samples):\n",
    "        \n",
    "        # get random image\n",
    "        idx = np.random.randint(len(dataset))\n",
    "        # load data\n",
    "        data = dataset[idx]\n",
    "        s2 = data[\"sentinel2\"]    # shape of s2 is (C, H, W)  // (12, 256, 256) \n",
    "    \n",
    "        # plot original\n",
    "        display_s2_image(s2, ax=ax[i, 0])\n",
    "        ax[i, 0].set_title(\"original\")\n",
    "        \n",
    "        # getting ready for transform by reading as tensor and adding batch dimension\n",
    "        s2_torch = torch.from_numpy(s2).unsqueeze(0)   # shape of this: (1,12, 256, 256)\n",
    "    return s2_torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc86c5-408f-4a56-a6d2-081742d7986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "     # CHANGE IF REQUIRED TO APPLY TRANSFORMS \n",
    "        for j, transform in enumerate(transform_list):\n",
    "        s2_transformed = transform(s2_torch)\n",
    "        # back to numpy\n",
    "        s2_transformed = s2_transformed[0] # only first view\n",
    "        s2_transformed = s2_transformed[0].numpy() # remove batch dim and convert to numpy\n",
    "\n",
    "        # plot transform\n",
    "        display_s2_image(s2_transformed, ax=ax[i, 1 + j])\n",
    "        name = transform.__class__.__name__.split(\".\")[-1]\n",
    "        ax[i, 1 + j].set_title(name)  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d45ec0f9-09a0-4bdb-b25c-999422f4c969",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valdir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# data loader - SIMPLE EXAMPLE\u001b[39;00m\n\u001b[1;32m      3\u001b[0m normalize \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m],\n\u001b[1;32m      4\u001b[0m                                      std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m      6\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m----> 7\u001b[0m         datasets\u001b[38;5;241m.\u001b[39mImageFolder(\u001b[43mvaldir\u001b[49m, transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      8\u001b[0m             transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m256\u001b[39m),\n\u001b[1;32m      9\u001b[0m             transforms\u001b[38;5;241m.\u001b[39mCenterCrop(\u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m     10\u001b[0m             transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     11\u001b[0m             normalize,\n\u001b[1;32m     12\u001b[0m         ])),\n\u001b[1;32m     13\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mworkers, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valdir' is not defined"
     ]
    }
   ],
   "source": [
    "# data loader - SIMPLE EXAMPLE\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "                                     \n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(valdir, transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=args.batch_size, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70be33c4-dd99-4d37-9595-83847a2d7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader - EXTENDED EXAMPLE\n",
    "\n",
    "\n",
    "\n",
    "# Define normalization specific to ImageNet\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Define the transformations for the validation set\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),        # Resize the smaller edge of the image to 256\n",
    "    transforms.CenterCrop(224),    # Crop the center 224x224 region from the resized image\n",
    "    transforms.ToTensor(),         # Convert the image to a PyTorch tensor\n",
    "    normalize,                     # Normalize the image with mean and std specific to ImageNet\n",
    "])\n",
    "\n",
    "# Create the dataset for validation\n",
    "val_dataset = datasets.ImageFolder(valdir, transform=val_transforms)\n",
    "\n",
    "# Create the DataLoader for validation\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.batch_size,   # Batch size\n",
    "    shuffle=False,                # Shuffle is set to False for validation data\n",
    "    num_workers=args.workers,     # Number of subprocesses to use for data loading\n",
    "    pin_memory=True               # Copy Tensors into CUDA pinned memory\n",
    ")\n",
    "\n",
    "# Example: iterating through the DataLoader\n",
    "for inputs, labels in val_loader:\n",
    "    # Forward pass, compute the loss, etc.\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSL4EO",
   "language": "python",
   "name": "ipy39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
